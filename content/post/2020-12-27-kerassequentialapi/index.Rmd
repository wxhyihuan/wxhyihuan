---
title:  Keras机器学习(3)-Sequential API介绍
author: wxhyihuan
date: '2020-12-27'
slug: kerassequentialapi
output:
  blogdown::html_page:
    toc: true
    toc_depth: 3
    fig_width: 6
    dev: "svg"
categories:
  - deep learning
  - Keras
tags:
  - R
  - Keras
---

# Sequential 模型指南

## Defining a Model

Sequential模型是一些列层的线性叠加。

通过调用keras_model_sequential()函数，然后调用一系列层函数来创建顺序模型:

```r 
library(keras)

model <- keras_model_sequential()
model %>% 
    layer_dense(units=32,input_shape=c(784)) %>% 
    layer_activation('relu') %>%
    layer_dense(units=10) %>%
    layer_activation('softmax')

```
请注意，Keras对象是在适当的地方修改的([Modifid in place])，这就是为什么在添加层之后没有必要将模型分配回给它。

使用summary()函数打印模型结构的摘要:
```r 
summary(model)
## Model: "sequential"
## ________________________________________________________________________________
## Layer (type)                        Output Shape                    Param #
## ================================================================================
## dense_1 (Dense)                     (None, 32)                      25120
## ________________________________________________________________________________
## activation_1 (Activation)           (None, 32)                      0
## ________________________________________________________________________________
## dense (Dense)                       (None, 10)                      330
## ________________________________________________________________________________
## activation (Activation)             (None, 10)                      0
## ================================================================================
## Total params: 25,450
## Trainable params: 25,450
## Non-trainable params: 0
## ________________________________________________________________________________
```

### INPUT SHAPES

模型需要知道它期望的输入形状是什么。因此，sequential模型中的第一层(只有第一层，因为后面的层可以进行自动形状推断)需要接收有关其输入形状的信息。

如上例所示，这是通过向第一层传递一个input_shape参数来实现的。这是一个整数或NULL的列表，其中NULL表示可以期望任何正整数。在input_shape中，batch 的维度不包括在内。

如果您需要为输入指定一个固定的batch大小(这对有状态循环网络很有用)，你可以传递一个batch_size参数给一个层。如果您将batch_size=32和input_shape=c(6,8)传递给一个层，那么它将期望每个batch输入都具有批形状(batch shape)为(32,6,8)。

## Compilation

在训练模型之前，您需要配置学习过程，这是通过compile()函数完成的。它接收三个参数:

- 一个优化器。这可能是一个现有优化器的字符串标识符(例如“rmsprop”或“adagrad”)或一个优化器函数的调用(例如optimizer_sgd())。

- 一个损失函数。这是模型试图最小化的目标。它可以是现有丢失函数的字符串标识符(例如" categorical_crossenropy"或"mse")或丢失函数的调用(例如loss_mean_squared_error())。

- 度量标准的列表。对于任何分类问题，您都希望将其设置为metrics = c('accuracy')。度量可以是现有度量的字符串标识符，也可以是对度量函数的调用(例如metric_binary_crossentropy())。

下面是模型的定义以及编译步骤(示例的compile()函数设置的参数适合于多类分类问题的):
```r 
# For a multi-class classification problem
model <- keras_model_sequential()
model %>% 
  layer_dense(units = 32, input_shapec(784)) %>% 
  layer_activation('relu') %>% 
  layer_dense(units = 10) %>% 
  layer_activation( 'softmax' )

model %>% compile(
  optimizer = 'rmsprop',
  loss = 'categorical_crossentropy',
  metrics = c('accuracy')
)

```

以下是均值平方误差回归问题的编译过程:

```r 
model %>% compile(
  optimizer = optimizer_rmsprop(lr = 0.002),
  loss = 'mse'
)
```
以下是一个二元分类问题的编译:
```r 
model %>% compile( 
  optimizer = optimizer_rmsprop(),
  loss = loss_binary_crossentropy,
  metrics = metric_binary_accuracy
)
```
下面是使用自定义度量的编译:
```r 
# create metric using backend tensor functions
metric_mean_pred <- custom_metric("mean_pred", function(y_true, y_pred) {
  k_mean(y_pred) 
})

model %>% compile( 
  optimizer = optimizer_rmsprop(),
  loss = loss_binary_crossentropy,
  metrics = c('accuracy', metric_mean_pred)
)
```

## Training

Keras模型是在R矩阵或输入数据和标签的高维数组上训练的。为了训练模型，您通常会使用fit()函数。

这是一个带有两个类(二元分类)的单输入模型:
```r 
# create model
model <- keras_model_sequential()

# add layers and compile the model
model %>% 
  layer_dense(units = 32, activation = 'relu', input_shape = c(100)) %>% 
  layer_dense(units = 1, activation = 'sigmoid') %>% 
  compile(
    optimizer = 'rmsprop',
    loss = 'binary_crossentropy',
    metrics = c('accuracy')
  )

# Generate dummy data
data <- matrix(runif(1000*100), nrow = 1000, ncol = 100)
labels <- matrix(round(runif(1000, min = 0, max = 1)), nrow = 1000, ncol = 1)

# Train the model, iterating on the data in batches of 32 samples
model %>% fit(data, labels, epochs=10, batch_size=32)
```

## Examples

这里有一些例子让你开始，在示例页面上，你还可以找到真实数据集的示例模型:

- [CIFAR10 small images classification]

- [IMDB movie review sentiment classification]

- [Reuters newswires topic classification]

- [MNIST handwritten digits classification]

- [Character-level text generation with LSTM]

### CIFAR10 small images classification

在CIFAR10小图像数据集上训练一个简单的深度CNN。模型在25个epach内logloss下降到0.65，在50个epach后下降到0.55，尽管这仍未达到要求。

```r 


library(keras)

# Parameters --------------------------------------------------------------

batch_size <- 32  # 设置 mini batch的容量
epochs <- 200     # 设置训练集epach的次数
data_augmentation <- TRUE


# Data Preparation --------------------------------------------------------

# See ?dataset_cifar10 for more info
cifar10 <- dataset_cifar10()

# Feature scale RGB values in test and train inputs  
x_train <- cifar10$train$x / 255
x_test <- cifar10$test$x / 255
y_train <- to_categorical(cifar10$train$y, num_classes = 10)
y_test <- to_categorical(cifar10$test$y, num_classes = 10)


# Defining Model ----------------------------------------------------------

# Initialize sequential model
model <- keras_model_sequential()

model %>%

# Start with hidden 2D convolutional layer being fed 32x32 pixel images
layer_conv_2d(
    filter = 32, kernel_size = c(3, 3), padding = "same",
    input_shape = c(32, 32, 3)
  ) %>%
  layer_activation("relu") %>%

# Second hidden layer
layer_conv_2d(filter = 32, kernel_size = c(3, 3)) %>%
  layer_activation("relu") %>%

# Use max pooling
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_dropout(0.25) %>%

# 2 additional hidden 2D convolutional layers
layer_conv_2d(filter = 32, kernel_size = c(3, 3), padding = "same") %>%
  layer_activation("relu") %>%
  layer_conv_2d(filter = 32, kernel_size = c(3, 3)) %>%
  layer_activation("relu") %>%

# Use max pooling once more
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_dropout(0.25) %>%

# Flatten max filtered output into feature vector 
# and feed into dense layer
layer_flatten() %>%
  layer_dense(512) %>%
  layer_activation("relu") %>%
  layer_dropout(0.5) %>%

# Outputs from dense layer are projected onto 10 unit output layer
layer_dense(10) %>%
  layer_activation("softmax")

opt <- optimizer_rmsprop(lr = 0.0001, decay = 1e-6)

model %>% compile(
  loss = "categorical_crossentropy",
  optimizer = opt,
  metrics = "accuracy"
)


# Training ----------------------------------------------------------------

if (!data_augmentation) {

  model %>% fit(
    x_train, y_train,
    batch_size = batch_size,
    epochs = epochs,
    validation_data = list(x_test, y_test),
    shuffle = TRUE
  )

} else {

  datagen <- image_data_generator(
    rotation_range = 20,
    width_shift_range = 0.2,
    height_shift_range = 0.2,
    horizontal_flip = TRUE
  )

  datagen %>% fit_image_data_generator(x_train)

  model %>% fit_generator(
    flow_images_from_data(x_train, y_train, datagen, batch_size = batch_size),
    steps_per_epoch = as.integer(50000 / batch_size),
    epochs = epochs,
    validation_data = list(x_test, y_test)
  )

}
```

### IMDB movie review sentiment classification

在IMDB文本倾向性分析(Sentiment Classification)上训练一个递归卷积网络。

```r 
library(keras)

# Parameters --------------------------------------------------------------

# Embedding
max_features = 20000
maxlen = 100
embedding_size = 128

# Convolution
kernel_size = 5
filters = 64
pool_size = 4

# LSTM
lstm_output_size = 70

# Training
batch_size = 30
epochs = 2

# Data Preparation --------------------------------------------------------

# The x data includes integer sequences, each integer is a word
# The y data includes a set of integer labels (0 or 1)
# The num_words argument indicates that only the max_fetures most frequent
# words will be integerized. All other will be ignored.
# See help(dataset_imdb)
imdb <- dataset_imdb(num_words = max_features)
# Keras load all data into a list with the following structure:
str(imdb)

# Pad the sequences to the same length
  # This will convert our dataset into a matrix: each line is a review
  # and each column a word on the sequence
# We pad the sequences with 0s to the left
x_train <- imdb$train$x %>%
  pad_sequences(maxlen = maxlen)
x_test <- imdb$test$x %>%
  pad_sequences(maxlen = maxlen)

# Defining Model ------------------------------------------------------

model <- keras_model_sequential()

model %>%
  layer_embedding(max_features, embedding_size, input_length = maxlen) %>%
  layer_dropout(0.25) %>%
  layer_conv_1d(
    filters, 
    kernel_size, 
    padding = "valid",
    activation = "relu",
    strides = 1
  ) %>%
  layer_max_pooling_1d(pool_size) %>%
  layer_lstm(lstm_output_size) %>%
  layer_dense(1) %>%
  layer_activation("sigmoid")

model %>% compile(
  loss = "binary_crossentropy",
  optimizer = "adam",
  metrics = "accuracy"
)

# Training ----------------------------------------------------------------

model %>% fit(
  x_train, imdb$train$y,
  batch_size = batch_size,
  epochs = epochs,
  validation_data = list(x_test, imdb$test$y)
)
```

### Reuters newswires topic classification

训练和评估一个简单的多层感知器(Multilayer Perceptron, MLP)对路透社新闻专线主题分类任务:

```r 

library(keras)

max_words <- 1000
batch_size <- 32
epochs <- 5

cat('Loading data...\n')
reuters <- dataset_reuters(num_words = max_words, test_split = 0.2)
x_train <- reuters$train$x
y_train <- reuters$train$y
x_test <- reuters$test$x
y_test <- reuters$test$y

cat(length(x_train), 'train sequences\n')
cat(length(x_test), 'test sequences\n')

num_classes <- max(y_train) + 1
cat(num_classes, '\n')

cat('Vectorizing sequence data...\n')

tokenizer <- text_tokenizer(num_words = max_words)
x_train <- sequences_to_matrix(tokenizer, x_train, mode = 'binary')
x_test <- sequences_to_matrix(tokenizer, x_test, mode = 'binary')

cat('x_train shape:', dim(x_train), '\n')
cat('x_test shape:', dim(x_test), '\n')

cat('Convert class vector to binary class matrix',
    '(for use with categorical_crossentropy)\n')
y_train <- to_categorical(y_train, num_classes)
y_test <- to_categorical(y_test, num_classes)
cat('y_train shape:', dim(y_train), '\n')
cat('y_test shape:', dim(y_test), '\n')

cat('Building model...\n')
model <- keras_model_sequential()
model %>%
  layer_dense(units = 512, input_shape = c(max_words)) %>%
  layer_activation(activation = 'relu') %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = num_classes) %>%
  layer_activation(activation = 'softmax')

model %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = 'adam',
  metrics = c('accuracy')
)

history <- model %>% fit(
  x_train, y_train,
  batch_size = batch_size,
  epochs = epochs,
  verbose = 1,
  validation_split = 0.1
)

score <- model %>% evaluate(
  x_test, y_test,
  batch_size = batch_size,
  verbose = 1
)

cat('Test score:', score[[1]], '\n')
cat('Test accuracy', score[[2]], '\n')
```

### MNIST handwritten digits classification

在MNIST数据集上训练一个简单的深度神经网络。

```r 
library(keras)

# Data Preparation ---------------------------------------------------

batch_size <- 128
num_classes <- 10
epochs <- 30

# The data, shuffled and split between train and test sets
c(c(x_train, y_train), c(x_test, y_test)) %<-% dataset_mnist()

x_train <- array_reshape(x_train, c(nrow(x_train), 784))
x_test <- array_reshape(x_test, c(nrow(x_test), 784))

# Transform RGB values into [0,1] range
x_train <- x_train / 255
x_test <- x_test / 255

cat(nrow(x_train), 'train samples\n')
cat(nrow(x_test), 'test samples\n')

# Convert class vectors to binary class matrices
y_train <- to_categorical(y_train, num_classes)
y_test <- to_categorical(y_test, num_classes)

# Define Model --------------------------------------------------------------

model <- keras_model_sequential()
model %>% 
  layer_dense(units = 256, activation = 'relu', input_shape = c(784)) %>% 
  layer_dropout(rate = 0.4) %>% 
  layer_dense(units = 128, activation = 'relu') %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 10, activation = 'softmax')

summary(model)

model %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

# Training & Evaluation ----------------------------------------------------

# Fit model to data
history <- model %>% fit(
  x_train, y_train,
  batch_size = batch_size,
  epochs = epochs,
  verbose = 1,
  validation_split = 0.2
)

plot(history)
  
score <- model %>% evaluate(
  x_test, y_test,
  verbose = 0
)

# Output metrics
cat('Test loss:', score[[1]], '\n')
cat('Test accuracy:', score[[2]], '\n')
```

### Character-level text generation with LSTM

从尼采(Nietzsche)的著作中生成文本的示例脚本。

在生成的文本开始听起来连贯之前，至少需要20个epochs。

建议在GPU上运行这个脚本，因为循环网络是相当密集的计算。

如果您在新数据上尝试这个脚本，请确保您的语料库至少有100k个字符，接近100W就是更好的。

```r 
library(keras)
library(readr)
library(stringr)
library(purrr)
library(tokenizers)

# Parameters --------------------------------------------------------------

maxlen <- 40

# Data Preparation --------------------------------------------------------

# Retrieve text
#path <- get_file(
#  'nietzsche.txt', 
#  origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt'
#  )

path <- get_file(
'HongLou.txt', 
origin='D://AppDatas//GitCode4vsce//TensorflowTrain//TFwithR//TFwithR//Chart01.txt'
)

# Load, collapse, and tokenize text
text <- read_lines(path) %>%
  str_to_lower() %>%
  str_c(collapse = "\n") %>%
  tokenize_characters(strip_non_alphanum = FALSE, simplify = TRUE)

print(sprintf("corpus length: %d", length(text)))

chars <- text %>%
  unique() %>%
  sort()

print(sprintf("total chars: %d", length(chars)))  

# Cut the text in semi-redundant sequences of maxlen characters
dataset <- map(
  seq(1, length(text) - maxlen - 1, by = 3), 
  ~list(sentece = text[.x:(.x + maxlen - 1)], next_char = text[.x + maxlen])
  )

dataset <- transpose(dataset)

# Vectorization
x <- array(0, dim = c(length(dataset$sentece), maxlen, length(chars)))
y <- array(0, dim = c(length(dataset$sentece), length(chars)))

for(i in 1:length(dataset$sentece)){
  
  x[i,,] <- sapply(chars, function(x){
    as.integer(x == dataset$sentece[[i]])
  })
  
  y[i,] <- as.integer(chars == dataset$next_char[[i]])
  
}

# Model Definition --------------------------------------------------------

model <- keras_model_sequential()

model %>%
  layer_lstm(128, input_shape = c(maxlen, length(chars))) %>%
  layer_dense(length(chars)) %>%
  layer_activation("softmax")

optimizer <- optimizer_rmsprop(lr = 0.01)

model %>% compile(
  loss = "categorical_crossentropy", 
  optimizer = optimizer
)

# Training & Results ----------------------------------------------------

sample_mod <- function(preds, temperature = 1){
  preds <- log(preds)/temperature
  exp_preds <- exp(preds)
  preds <- exp_preds/sum(exp(preds))
  
  rmultinom(1, 1, preds) %>% 
    as.integer() %>%
    which.max()
}

on_epoch_end <- function(epoch, logs) {
  
  cat(sprintf("epoch: %02d ---------------\n\n", epoch))
  
  for(diversity in c(0.2, 0.5, 1, 1.2)){
    
    cat(sprintf("diversity: %f ---------------\n\n", diversity))
    
    start_index <- sample(1:(length(text) - maxlen), size = 1)
    sentence <- text[start_index:(start_index + maxlen - 1)]
    generated <- ""
    
    for(i in 1:400){
      
      x <- sapply(chars, function(x){
        as.integer(x == sentence)
      })
      x <- array_reshape(x, c(1, dim(x)))
      
      preds <- predict(model, x)
      next_index <- sample_mod(preds, diversity)
      next_char <- chars[next_index]
      
      generated <- str_c(generated, next_char, collapse = "")
      sentence <- c(sentence[-1], next_char)
      
    }
    
    cat(generated)
    cat("\n\n")
    
  }
}

print_callback <- callback_lambda(on_epoch_end = on_epoch_end)

model %>% fit(
  x, y,
  batch_size = 128,
  epochs = 1,
  callbacks = print_callback
)
```

下面还提供了一些额外的示例。

### MULTILAYER PERCEPTRON (MLP) FOR MULTI-CLASS SOFTMAX CLASSIFICATION

[CIFAR10 small images classification]:https://tensorflow.rstudio.com/guide/keras/examples/cifar10_cnn
[IMDB movie review sentiment classification]:https://tensorflow.rstudio.com/guide/keras/examples/imdb_cnn_lstm
[Reuters newswires topic classification]:https://tensorflow.rstudio.com/guide/keras/examples/reuters_mlp
[MNIST handwritten digits classification]:https://tensorflow.rstudio.com/guide/keras/examples/mnist_mlp
[Character-level text generation with LSTM]:https://tensorflow.rstudio.com/guide/keras/examples/lstm_text_generation
[Modifid in place]:https://tensorflow.rstudio.com/guide/keras/sequential_model/faq.html#why-are-keras-objects-modified-in-place
