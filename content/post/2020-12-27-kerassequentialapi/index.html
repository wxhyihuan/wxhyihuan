---
title:  Keras机器学习(3)-Sequential API介绍
author: wxhyihuan
date: '2020-12-27'
slug: kerassequentialapi
output:
  blogdown::html_page:
    toc: true
    toc_depth: 3
    fig_width: 6
    dev: "svg"
categories:
  - deep learning
  - Keras
tags:
  - R
  - Keras
---

<link href="{{< blogdown/postref >}}index_files/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="{{< blogdown/postref >}}index_files/anchor-sections/anchor-sections.js"></script>

<div id="TOC">
<ul>
<li><a href="#sequential-模型指南">Sequential 模型指南</a><ul>
<li><a href="#defining-a-model">Defining a Model</a><ul>
<li><a href="#input-shapes">INPUT SHAPES</a></li>
</ul></li>
<li><a href="#compilation">Compilation</a></li>
<li><a href="#training">Training</a></li>
<li><a href="#examples">Examples</a><ul>
<li><a href="#cifar10-small-images-classification">CIFAR10 small images classification</a></li>
<li><a href="#imdb-movie-review-sentiment-classification">IMDB movie review sentiment classification</a></li>
<li><a href="#reuters-newswires-topic-classification">Reuters newswires topic classification</a></li>
<li><a href="#mnist-handwritten-digits-classification">MNIST handwritten digits classification</a></li>
<li><a href="#character-level-text-generation-with-lstm">Character-level text generation with LSTM</a></li>
<li><a href="#multilayer-perceptron-mlp-for-multi-class-softmax-classification">MULTILAYER PERCEPTRON (MLP) FOR MULTI-CLASS SOFTMAX CLASSIFICATION</a></li>
</ul></li>
</ul></li>
</ul>
</div>

<div id="sequential-模型指南" class="section level1">
<h1>Sequential 模型指南</h1>
<div id="defining-a-model" class="section level2">
<h2>Defining a Model</h2>
<p>Sequential模型是一些列层的线性叠加。</p>
<p>通过调用keras_model_sequential()函数，然后调用一系列层函数来创建顺序模型:</p>
<pre class="r"><code>library(keras)

model &lt;- keras_model_sequential()
model %&gt;% 
    layer_dense(units=32,input_shape=c(784)) %&gt;% 
    layer_activation(&#39;relu&#39;) %&gt;%
    layer_dense(units=10) %&gt;%
    layer_activation(&#39;softmax&#39;)
</code></pre>
<p>请注意，Keras对象是在适当的地方修改的(<a href="https://tensorflow.rstudio.com/guide/keras/sequential_model/faq.html#why-are-keras-objects-modified-in-place">Modifid in place</a>)，这就是为什么在添加层之后没有必要将模型分配回给它。</p>
<p>使用summary()函数打印模型结构的摘要:</p>
<pre class="r"><code>summary(model)
## Model: &quot;sequential&quot;
## ________________________________________________________________________________
## Layer (type)                        Output Shape                    Param #
## ================================================================================
## dense_1 (Dense)                     (None, 32)                      25120
## ________________________________________________________________________________
## activation_1 (Activation)           (None, 32)                      0
## ________________________________________________________________________________
## dense (Dense)                       (None, 10)                      330
## ________________________________________________________________________________
## activation (Activation)             (None, 10)                      0
## ================================================================================
## Total params: 25,450
## Trainable params: 25,450
## Non-trainable params: 0
## ________________________________________________________________________________</code></pre>
<div id="input-shapes" class="section level3">
<h3>INPUT SHAPES</h3>
<p>模型需要知道它期望的输入形状是什么。因此，sequential模型中的第一层(只有第一层，因为后面的层可以进行自动形状推断)需要接收有关其输入形状的信息。</p>
<p>如上例所示，这是通过向第一层传递一个input_shape参数来实现的。这是一个整数或NULL的列表，其中NULL表示可以期望任何正整数。在input_shape中，batch 的维度不包括在内。</p>
<p>如果您需要为输入指定一个固定的batch大小(这对有状态循环网络很有用)，你可以传递一个batch_size参数给一个层。如果您将batch_size=32和input_shape=c(6,8)传递给一个层，那么它将期望每个batch输入都具有批形状(batch shape)为(32,6,8)。</p>
</div>
</div>
<div id="compilation" class="section level2">
<h2>Compilation</h2>
<p>在训练模型之前，您需要配置学习过程，这是通过compile()函数完成的。它接收三个参数:</p>
<ul>
<li><p>一个优化器。这可能是一个现有优化器的字符串标识符(例如“rmsprop”或“adagrad”)或一个优化器函数的调用(例如optimizer_sgd())。</p></li>
<li><p>一个损失函数。这是模型试图最小化的目标。它可以是现有丢失函数的字符串标识符(例如" categorical_crossenropy“或”mse")或丢失函数的调用(例如loss_mean_squared_error())。</p></li>
<li><p>度量标准的列表。对于任何分类问题，您都希望将其设置为metrics = c(‘accuracy’)。度量可以是现有度量的字符串标识符，也可以是对度量函数的调用(例如metric_binary_crossentropy())。</p></li>
</ul>
<p>下面是模型的定义以及编译步骤(示例的compile()函数设置的参数适合于多类分类问题的):</p>
<pre class="r"><code># For a multi-class classification problem
model &lt;- keras_model_sequential()
model %&gt;% 
  layer_dense(units = 32, input_shapec(784)) %&gt;% 
  layer_activation(&#39;relu&#39;) %&gt;% 
  layer_dense(units = 10) %&gt;% 
  layer_activation( &#39;softmax&#39; )

model %&gt;% compile(
  optimizer = &#39;rmsprop&#39;,
  loss = &#39;categorical_crossentropy&#39;,
  metrics = c(&#39;accuracy&#39;)
)
</code></pre>
<p>以下是均值平方误差回归问题的编译过程:</p>
<pre class="r"><code>model %&gt;% compile(
  optimizer = optimizer_rmsprop(lr = 0.002),
  loss = &#39;mse&#39;
)</code></pre>
<p>以下是一个二元分类问题的编译:</p>
<pre class="r"><code>model %&gt;% compile( 
  optimizer = optimizer_rmsprop(),
  loss = loss_binary_crossentropy,
  metrics = metric_binary_accuracy
)</code></pre>
<p>下面是使用自定义度量的编译:</p>
<pre class="r"><code># create metric using backend tensor functions
metric_mean_pred &lt;- custom_metric(&quot;mean_pred&quot;, function(y_true, y_pred) {
  k_mean(y_pred) 
})

model %&gt;% compile( 
  optimizer = optimizer_rmsprop(),
  loss = loss_binary_crossentropy,
  metrics = c(&#39;accuracy&#39;, metric_mean_pred)
)</code></pre>
</div>
<div id="training" class="section level2">
<h2>Training</h2>
<p>Keras模型是在R矩阵或输入数据和标签的高维数组上训练的。为了训练模型，您通常会使用fit()函数。</p>
<p>这是一个带有两个类(二元分类)的单输入模型:</p>
<pre class="r"><code># create model
model &lt;- keras_model_sequential()

# add layers and compile the model
model %&gt;% 
  layer_dense(units = 32, activation = &#39;relu&#39;, input_shape = c(100)) %&gt;% 
  layer_dense(units = 1, activation = &#39;sigmoid&#39;) %&gt;% 
  compile(
    optimizer = &#39;rmsprop&#39;,
    loss = &#39;binary_crossentropy&#39;,
    metrics = c(&#39;accuracy&#39;)
  )

# Generate dummy data
data &lt;- matrix(runif(1000*100), nrow = 1000, ncol = 100)
labels &lt;- matrix(round(runif(1000, min = 0, max = 1)), nrow = 1000, ncol = 1)

# Train the model, iterating on the data in batches of 32 samples
model %&gt;% fit(data, labels, epochs=10, batch_size=32)</code></pre>
</div>
<div id="examples" class="section level2">
<h2>Examples</h2>
<p>这里有一些例子让你开始，在示例页面上，你还可以找到真实数据集的示例模型:</p>
<ul>
<li><p><a href="https://tensorflow.rstudio.com/guide/keras/examples/cifar10_cnn">CIFAR10 small images classification</a></p></li>
<li><p><a href="https://tensorflow.rstudio.com/guide/keras/examples/imdb_cnn_lstm">IMDB movie review sentiment classification</a></p></li>
<li><p><a href="https://tensorflow.rstudio.com/guide/keras/examples/reuters_mlp">Reuters newswires topic classification</a></p></li>
<li><p><a href="https://tensorflow.rstudio.com/guide/keras/examples/mnist_mlp">MNIST handwritten digits classification</a></p></li>
<li><p><a href="https://tensorflow.rstudio.com/guide/keras/examples/lstm_text_generation">Character-level text generation with LSTM</a></p></li>
</ul>
<div id="cifar10-small-images-classification" class="section level3">
<h3>CIFAR10 small images classification</h3>
<p>在CIFAR10小图像数据集上训练一个简单的深度CNN。模型在25个epach内logloss下降到0.65，在50个epach后下降到0.55，尽管这仍未达到要求。</p>
<pre class="r"><code>

library(keras)

# Parameters --------------------------------------------------------------

batch_size &lt;- 32  # 设置 mini batch的容量
epochs &lt;- 200     # 设置训练集epach的次数
data_augmentation &lt;- TRUE


# Data Preparation --------------------------------------------------------

# See ?dataset_cifar10 for more info
cifar10 &lt;- dataset_cifar10()

# Feature scale RGB values in test and train inputs  
x_train &lt;- cifar10$train$x / 255
x_test &lt;- cifar10$test$x / 255
y_train &lt;- to_categorical(cifar10$train$y, num_classes = 10)
y_test &lt;- to_categorical(cifar10$test$y, num_classes = 10)


# Defining Model ----------------------------------------------------------

# Initialize sequential model
model &lt;- keras_model_sequential()

model %&gt;%

# Start with hidden 2D convolutional layer being fed 32x32 pixel images
layer_conv_2d(
    filter = 32, kernel_size = c(3, 3), padding = &quot;same&quot;,
    input_shape = c(32, 32, 3)
  ) %&gt;%
  layer_activation(&quot;relu&quot;) %&gt;%

# Second hidden layer
layer_conv_2d(filter = 32, kernel_size = c(3, 3)) %&gt;%
  layer_activation(&quot;relu&quot;) %&gt;%

# Use max pooling
layer_max_pooling_2d(pool_size = c(2, 2)) %&gt;%
  layer_dropout(0.25) %&gt;%

# 2 additional hidden 2D convolutional layers
layer_conv_2d(filter = 32, kernel_size = c(3, 3), padding = &quot;same&quot;) %&gt;%
  layer_activation(&quot;relu&quot;) %&gt;%
  layer_conv_2d(filter = 32, kernel_size = c(3, 3)) %&gt;%
  layer_activation(&quot;relu&quot;) %&gt;%

# Use max pooling once more
layer_max_pooling_2d(pool_size = c(2, 2)) %&gt;%
  layer_dropout(0.25) %&gt;%

# Flatten max filtered output into feature vector 
# and feed into dense layer
layer_flatten() %&gt;%
  layer_dense(512) %&gt;%
  layer_activation(&quot;relu&quot;) %&gt;%
  layer_dropout(0.5) %&gt;%

# Outputs from dense layer are projected onto 10 unit output layer
layer_dense(10) %&gt;%
  layer_activation(&quot;softmax&quot;)

opt &lt;- optimizer_rmsprop(lr = 0.0001, decay = 1e-6)

model %&gt;% compile(
  loss = &quot;categorical_crossentropy&quot;,
  optimizer = opt,
  metrics = &quot;accuracy&quot;
)


# Training ----------------------------------------------------------------

if (!data_augmentation) {

  model %&gt;% fit(
    x_train, y_train,
    batch_size = batch_size,
    epochs = epochs,
    validation_data = list(x_test, y_test),
    shuffle = TRUE
  )

} else {

  datagen &lt;- image_data_generator(
    rotation_range = 20,
    width_shift_range = 0.2,
    height_shift_range = 0.2,
    horizontal_flip = TRUE
  )

  datagen %&gt;% fit_image_data_generator(x_train)

  model %&gt;% fit_generator(
    flow_images_from_data(x_train, y_train, datagen, batch_size = batch_size),
    steps_per_epoch = as.integer(50000 / batch_size),
    epochs = epochs,
    validation_data = list(x_test, y_test)
  )

}</code></pre>
</div>
<div id="imdb-movie-review-sentiment-classification" class="section level3">
<h3>IMDB movie review sentiment classification</h3>
<p>在IMDB文本倾向性分析(Sentiment Classification)上训练一个递归卷积网络。</p>
<pre class="r"><code>library(keras)

# Parameters --------------------------------------------------------------

# Embedding
max_features = 20000
maxlen = 100
embedding_size = 128

# Convolution
kernel_size = 5
filters = 64
pool_size = 4

# LSTM
lstm_output_size = 70

# Training
batch_size = 30
epochs = 2

# Data Preparation --------------------------------------------------------

# The x data includes integer sequences, each integer is a word
# The y data includes a set of integer labels (0 or 1)
# The num_words argument indicates that only the max_fetures most frequent
# words will be integerized. All other will be ignored.
# See help(dataset_imdb)
imdb &lt;- dataset_imdb(num_words = max_features)
# Keras load all data into a list with the following structure:
str(imdb)

# Pad the sequences to the same length
  # This will convert our dataset into a matrix: each line is a review
  # and each column a word on the sequence
# We pad the sequences with 0s to the left
x_train &lt;- imdb$train$x %&gt;%
  pad_sequences(maxlen = maxlen)
x_test &lt;- imdb$test$x %&gt;%
  pad_sequences(maxlen = maxlen)

# Defining Model ------------------------------------------------------

model &lt;- keras_model_sequential()

model %&gt;%
  layer_embedding(max_features, embedding_size, input_length = maxlen) %&gt;%
  layer_dropout(0.25) %&gt;%
  layer_conv_1d(
    filters, 
    kernel_size, 
    padding = &quot;valid&quot;,
    activation = &quot;relu&quot;,
    strides = 1
  ) %&gt;%
  layer_max_pooling_1d(pool_size) %&gt;%
  layer_lstm(lstm_output_size) %&gt;%
  layer_dense(1) %&gt;%
  layer_activation(&quot;sigmoid&quot;)

model %&gt;% compile(
  loss = &quot;binary_crossentropy&quot;,
  optimizer = &quot;adam&quot;,
  metrics = &quot;accuracy&quot;
)

# Training ----------------------------------------------------------------

model %&gt;% fit(
  x_train, imdb$train$y,
  batch_size = batch_size,
  epochs = epochs,
  validation_data = list(x_test, imdb$test$y)
)</code></pre>
</div>
<div id="reuters-newswires-topic-classification" class="section level3">
<h3>Reuters newswires topic classification</h3>
<p>训练和评估一个简单的多层感知器(Multilayer Perceptron, MLP)对路透社新闻专线主题分类任务:</p>
<pre class="r"><code>
library(keras)

max_words &lt;- 1000
batch_size &lt;- 32
epochs &lt;- 5

cat(&#39;Loading data...\n&#39;)
reuters &lt;- dataset_reuters(num_words = max_words, test_split = 0.2)
x_train &lt;- reuters$train$x
y_train &lt;- reuters$train$y
x_test &lt;- reuters$test$x
y_test &lt;- reuters$test$y

cat(length(x_train), &#39;train sequences\n&#39;)
cat(length(x_test), &#39;test sequences\n&#39;)

num_classes &lt;- max(y_train) + 1
cat(num_classes, &#39;\n&#39;)

cat(&#39;Vectorizing sequence data...\n&#39;)

tokenizer &lt;- text_tokenizer(num_words = max_words)
x_train &lt;- sequences_to_matrix(tokenizer, x_train, mode = &#39;binary&#39;)
x_test &lt;- sequences_to_matrix(tokenizer, x_test, mode = &#39;binary&#39;)

cat(&#39;x_train shape:&#39;, dim(x_train), &#39;\n&#39;)
cat(&#39;x_test shape:&#39;, dim(x_test), &#39;\n&#39;)

cat(&#39;Convert class vector to binary class matrix&#39;,
    &#39;(for use with categorical_crossentropy)\n&#39;)
y_train &lt;- to_categorical(y_train, num_classes)
y_test &lt;- to_categorical(y_test, num_classes)
cat(&#39;y_train shape:&#39;, dim(y_train), &#39;\n&#39;)
cat(&#39;y_test shape:&#39;, dim(y_test), &#39;\n&#39;)

cat(&#39;Building model...\n&#39;)
model &lt;- keras_model_sequential()
model %&gt;%
  layer_dense(units = 512, input_shape = c(max_words)) %&gt;%
  layer_activation(activation = &#39;relu&#39;) %&gt;%
  layer_dropout(rate = 0.5) %&gt;%
  layer_dense(units = num_classes) %&gt;%
  layer_activation(activation = &#39;softmax&#39;)

model %&gt;% compile(
  loss = &#39;categorical_crossentropy&#39;,
  optimizer = &#39;adam&#39;,
  metrics = c(&#39;accuracy&#39;)
)

history &lt;- model %&gt;% fit(
  x_train, y_train,
  batch_size = batch_size,
  epochs = epochs,
  verbose = 1,
  validation_split = 0.1
)

score &lt;- model %&gt;% evaluate(
  x_test, y_test,
  batch_size = batch_size,
  verbose = 1
)

cat(&#39;Test score:&#39;, score[[1]], &#39;\n&#39;)
cat(&#39;Test accuracy&#39;, score[[2]], &#39;\n&#39;)</code></pre>
</div>
<div id="mnist-handwritten-digits-classification" class="section level3">
<h3>MNIST handwritten digits classification</h3>
<p>在MNIST数据集上训练一个简单的深度神经网络。</p>
<pre class="r"><code>library(keras)

# Data Preparation ---------------------------------------------------

batch_size &lt;- 128
num_classes &lt;- 10
epochs &lt;- 30

# The data, shuffled and split between train and test sets
c(c(x_train, y_train), c(x_test, y_test)) %&lt;-% dataset_mnist()

x_train &lt;- array_reshape(x_train, c(nrow(x_train), 784))
x_test &lt;- array_reshape(x_test, c(nrow(x_test), 784))

# Transform RGB values into [0,1] range
x_train &lt;- x_train / 255
x_test &lt;- x_test / 255

cat(nrow(x_train), &#39;train samples\n&#39;)
cat(nrow(x_test), &#39;test samples\n&#39;)

# Convert class vectors to binary class matrices
y_train &lt;- to_categorical(y_train, num_classes)
y_test &lt;- to_categorical(y_test, num_classes)

# Define Model --------------------------------------------------------------

model &lt;- keras_model_sequential()
model %&gt;% 
  layer_dense(units = 256, activation = &#39;relu&#39;, input_shape = c(784)) %&gt;% 
  layer_dropout(rate = 0.4) %&gt;% 
  layer_dense(units = 128, activation = &#39;relu&#39;) %&gt;%
  layer_dropout(rate = 0.3) %&gt;%
  layer_dense(units = 10, activation = &#39;softmax&#39;)

summary(model)

model %&gt;% compile(
  loss = &#39;categorical_crossentropy&#39;,
  optimizer = optimizer_rmsprop(),
  metrics = c(&#39;accuracy&#39;)
)

# Training &amp; Evaluation ----------------------------------------------------

# Fit model to data
history &lt;- model %&gt;% fit(
  x_train, y_train,
  batch_size = batch_size,
  epochs = epochs,
  verbose = 1,
  validation_split = 0.2
)

plot(history)
  
score &lt;- model %&gt;% evaluate(
  x_test, y_test,
  verbose = 0
)

# Output metrics
cat(&#39;Test loss:&#39;, score[[1]], &#39;\n&#39;)
cat(&#39;Test accuracy:&#39;, score[[2]], &#39;\n&#39;)</code></pre>
</div>
<div id="character-level-text-generation-with-lstm" class="section level3">
<h3>Character-level text generation with LSTM</h3>
<p>从尼采(Nietzsche)的著作中生成文本的示例脚本。</p>
<p>在生成的文本开始听起来连贯之前，至少需要20个epochs。</p>
<p>建议在GPU上运行这个脚本，因为循环网络是相当密集的计算。</p>
<p>如果您在新数据上尝试这个脚本，请确保您的语料库至少有100k个字符，接近100W就是更好的。</p>
<pre class="r"><code>library(keras)
library(readr)
library(stringr)
library(purrr)
library(tokenizers)

# Parameters --------------------------------------------------------------

maxlen &lt;- 40

# Data Preparation --------------------------------------------------------

# Retrieve text
#path &lt;- get_file(
#  &#39;nietzsche.txt&#39;, 
#  origin=&#39;https://s3.amazonaws.com/text-datasets/nietzsche.txt&#39;
#  )

path &lt;- get_file(
&#39;HongLou.txt&#39;, 
origin=&#39;D://AppDatas//GitCode4vsce//TensorflowTrain//TFwithR//TFwithR//Chart01.txt&#39;
)

# Load, collapse, and tokenize text
text &lt;- read_lines(path) %&gt;%
  str_to_lower() %&gt;%
  str_c(collapse = &quot;\n&quot;) %&gt;%
  tokenize_characters(strip_non_alphanum = FALSE, simplify = TRUE)

print(sprintf(&quot;corpus length: %d&quot;, length(text)))

chars &lt;- text %&gt;%
  unique() %&gt;%
  sort()

print(sprintf(&quot;total chars: %d&quot;, length(chars)))  

# Cut the text in semi-redundant sequences of maxlen characters
dataset &lt;- map(
  seq(1, length(text) - maxlen - 1, by = 3), 
  ~list(sentece = text[.x:(.x + maxlen - 1)], next_char = text[.x + maxlen])
  )

dataset &lt;- transpose(dataset)

# Vectorization
x &lt;- array(0, dim = c(length(dataset$sentece), maxlen, length(chars)))
y &lt;- array(0, dim = c(length(dataset$sentece), length(chars)))

for(i in 1:length(dataset$sentece)){
  
  x[i,,] &lt;- sapply(chars, function(x){
    as.integer(x == dataset$sentece[[i]])
  })
  
  y[i,] &lt;- as.integer(chars == dataset$next_char[[i]])
  
}

# Model Definition --------------------------------------------------------

model &lt;- keras_model_sequential()

model %&gt;%
  layer_lstm(128, input_shape = c(maxlen, length(chars))) %&gt;%
  layer_dense(length(chars)) %&gt;%
  layer_activation(&quot;softmax&quot;)

optimizer &lt;- optimizer_rmsprop(lr = 0.01)

model %&gt;% compile(
  loss = &quot;categorical_crossentropy&quot;, 
  optimizer = optimizer
)

# Training &amp; Results ----------------------------------------------------

sample_mod &lt;- function(preds, temperature = 1){
  preds &lt;- log(preds)/temperature
  exp_preds &lt;- exp(preds)
  preds &lt;- exp_preds/sum(exp(preds))
  
  rmultinom(1, 1, preds) %&gt;% 
    as.integer() %&gt;%
    which.max()
}

on_epoch_end &lt;- function(epoch, logs) {
  
  cat(sprintf(&quot;epoch: %02d ---------------\n\n&quot;, epoch))
  
  for(diversity in c(0.2, 0.5, 1, 1.2)){
    
    cat(sprintf(&quot;diversity: %f ---------------\n\n&quot;, diversity))
    
    start_index &lt;- sample(1:(length(text) - maxlen), size = 1)
    sentence &lt;- text[start_index:(start_index + maxlen - 1)]
    generated &lt;- &quot;&quot;
    
    for(i in 1:400){
      
      x &lt;- sapply(chars, function(x){
        as.integer(x == sentence)
      })
      x &lt;- array_reshape(x, c(1, dim(x)))
      
      preds &lt;- predict(model, x)
      next_index &lt;- sample_mod(preds, diversity)
      next_char &lt;- chars[next_index]
      
      generated &lt;- str_c(generated, next_char, collapse = &quot;&quot;)
      sentence &lt;- c(sentence[-1], next_char)
      
    }
    
    cat(generated)
    cat(&quot;\n\n&quot;)
    
  }
}

print_callback &lt;- callback_lambda(on_epoch_end = on_epoch_end)

model %&gt;% fit(
  x, y,
  batch_size = 128,
  epochs = 1,
  callbacks = print_callback
)</code></pre>
<p>下面还提供了一些额外的示例。</p>
</div>
<div id="multilayer-perceptron-mlp-for-multi-class-softmax-classification" class="section level3">
<h3>MULTILAYER PERCEPTRON (MLP) FOR MULTI-CLASS SOFTMAX CLASSIFICATION</h3>
</div>
</div>
</div>
